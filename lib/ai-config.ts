/**
 * ğŸ¤– AI Configuration - Centro de Control de Sophia
 *
 * Este archivo centraliza TODAS las configuraciones relacionadas con AI/LLM en el sistema.
 *
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * REGLA OBLIGATORIA:
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * Cualquier valor relacionado con modelos, tokens, lÃ­mites, o comportamiento
 * de IA DEBE estar definido aquÃ­. NO hardcodear valores en otros archivos.
 *
 * Ver CLAUDE.md secciÃ³n "ğŸ¤– AI Config Protocol" para workflow completo.
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * ARQUITECTURA DE MEMORIA:
 *
 * Sophia usa una arquitectura de doble-prompt con memoria conversacional:
 *
 *   1. VERIFICATION PROMPT (ejecutado PRIMERO)
 *      - EvalÃºa si el estudiante completÃ³ los criterios de la actividad
 *      - SIN historial conversacional (optimizaciÃ³n de tokens)
 *      - Output: JSON estructurado con completed/criteriaMatched/feedback
 *
 *   2. SYSTEM PROMPT (ejecutado DESPUÃ‰S)
 *      - Usa el resultado de verificaciÃ³n para dar feedback personalizado
 *      - SIN historial conversacional (instrucciones estÃ¡ticas)
 *      - Recibe contexto pedagÃ³gico de la actividad actual
 *
 *   3. MESSAGES ARRAY (memoria real del sistema)
 *      - Claude recibe Ãºltimos N mensajes vÃ­a parÃ¡metro 'messages'
 *      - Mantiene coherencia conversacional completa
 *      - Esta es la VERDADERA memoria del sistema
 *
 * Flujo completo:
 *   User Input â†’ Verification (evalÃºa) â†’ System Prompt (instruye) â†’ Claude (responde con memoria)
 *                                                                        â†‘
 *                                                          messages: [...Ãºltimos N mensajes]
 *
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */

export const AI_CONFIG = {
  /**
   * MODELOS DE ANTHROPIC
   *
   * Sophia usa un mix de modelos de ANTHROPIC.
   *
   * Consideraciones:
   * - Sonnet 4.5: Balance Ã³ptimo entre calidad y costo
   * - Haiku: OpciÃ³n mÃ¡s econÃ³mica para futuro (verificaciÃ³n simple)
   * - Opus: OpciÃ³n premium para casos que requieran mÃ¡xima calidad
   *
   * IMPORTANTE: Cambiar modelo afecta:
   * - Costo por request (input + output tokens)
   * - Latencia de respuesta
   * - Calidad de evaluaciÃ³n/respuesta
   */
  models: {
    /**
     * Modelo para chat streaming (respuestas principales del instructor)
     *
     * Usado en: app/api/chat/stream/route.ts
     * Contexto: ConversaciÃ³n principal con el estudiante
     * Input tokens tÃ­pico: 2000-3000 (system prompt + historial + user message)
     * Output tokens tÃ­pico: 500-1024
     */
    chat: 'claude-sonnet-4-5-20250929',

    /**
     * Modelo para verificaciÃ³n de actividades
     *
     * Usado en: lib/activity-verification.ts
     * Contexto: Evaluar si estudiante completÃ³ criterios (output JSON)
     * Input tokens tÃ­pico: 300-500 (criteria + user message, SIN historial)
     * Output tokens tÃ­pico: 150-300 (JSON estructurado)
     *
     * OPTIMIZACIÃ“N: Este modelo es candidato para downgrade a Haiku
     * ya que solo necesita evaluar criterios simples (no creatividad).
     */
    // verification: 'claude-sonnet-4-5-20250929',
    verification: 'claude-haiku-4-5-20251001',

    /**
     * Modelo para mensaje de bienvenida
     *
     * Usado en: app/api/chat/welcome/route.ts
     * Contexto: Primera interacciÃ³n al iniciar lecciÃ³n
     * Input tokens tÃ­pico: 800-1200 (full system prompt con pedagogÃ­a)
     * Output tokens tÃ­pico: 200-512
     */
    // welcome: 'claude-sonnet-4-5-20250929',
    welcome: 'claude-haiku-4-5-20251001',
  },

  /**
   * LÃMITES DE TOKENS (max_tokens)
   *
   * Controla la longitud mÃ¡xima de las respuestas de Claude.
   *
   * Trade-offs:
   * - MÃ¡s tokens = Respuestas mÃ¡s completas pero mÃ¡s costo y latencia
   * - Menos tokens = Respuestas concisas pero riesgo de corte abrupto
   *
   * IMPACTO EN COSTOS:
   * Estos valores son multiplicadores directos del costo. Cada token de
   * output es ~4x mÃ¡s caro que input en Sonnet 4.5.
   */
  tokens: {
    /**
     * Max tokens para respuestas de chat
     *
     * Valor actual: 1024 tokens (~750-800 palabras en espaÃ±ol)
     *
     * Contexto: Respuestas conversacionales del instructor
     * GuÃ­a en prompt: "mÃ¡ximo 3-4 pÃ¡rrafos"
     *
     * Ajuste recomendado:
     * - 512: Respuestas muy concisas (2 pÃ¡rrafos)
     * - 1024: Balance actual (3-4 pÃ¡rrafos) âœ…
     * - 2048: Respuestas largas (explicaciones detalladas)
     */
    chat: 1024,

    /**
     * Max tokens para verificaciÃ³n de actividades
     *
     * Valor actual: 500 tokens
     *
     * Contexto: Output JSON estructurado (completed, criteriaMatched, feedback)
     * TÃ­picamente usa: 150-300 tokens reales
     *
     * Este valor es generoso para garantizar que el JSON completo
     * siempre se genere correctamente sin corte.
     */
    verification: 500,

    /**
     * Max tokens para mensaje de bienvenida
     *
     * Valor actual: 512 tokens (~380-400 palabras)
     *
     * Contexto: Primera interacciÃ³n, establece tono y engagement
     * Mayor que chat normal porque:
     * - Debe ser mÃ¡s cÃ¡lido y atractivo
     * - Incluye introducciÃ³n al tema
     * - Hace pregunta de engagement inicial
     */
    welcome: 512,
  },

  /**
   * RATE LIMITING
   *
   * ProtecciÃ³n anti-spam y control de costos por usuario.
   *
   * Implementado con sliding window en memoria (Map).
   * Ver: lib/rate-limit.ts
   */
  rateLimit: {
    /**
     * Mensajes permitidos por ventana de tiempo
     *
     * Valor actual: 10 mensajes
     *
     * Balance:
     * - Muy bajo (<5): Frustra usuarios legÃ­timos en conversaciÃ³n fluida
     * - Balance (10): Permite conversaciÃ³n natural, previene spam âœ…
     * - Muy alto (>20): No previene abuso efectivamente
     *
     * IMPORTANTE: En producciÃ³n, este lÃ­mite protege contra:
     * - Usuarios maliciosos (ataques de costo)
     * - Bugs de frontend (loops infinitos)
     * - Uso accidental excesivo
     */
    messagesPerMinute: 10,

    /**
     * Ventana de tiempo para rate limiting
     *
     * Valor actual: 60 segundos (1 minuto)
     *
     * Se usa sliding window, no fixed window.
     * Ejemplo: Si envÃ­as 10 mensajes en 30s, debes esperar 30s mÃ¡s
     * (no esperar al minuto completo).
     */
    windowSeconds: 60,

    /**
     * Intervalo de limpieza de rate limit cache
     *
     * Valor actual: 5 minutos (300000 ms)
     *
     * Limpia entradas expiradas del Map en memoria para prevenir
     * memory leaks en instancias de larga duraciÃ³n.
     *
     * No afecta UX, solo mantenimiento interno.
     */
    cleanupIntervalMs: 5 * 60 * 1000,
  },

  /**
   * HISTORIAL CONVERSACIONAL
   *
   * CRÃTICO: Esta es la configuraciÃ³n que define la "memoria" del sistema.
   *
   * ARQUITECTURA:
   * - Prompts (verification + system): NO incluyen historial (optimizaciÃ³n)
   * - Messages array: SÃ incluye historial (memoria real de Claude)
   *
   * Claude ve los Ãºltimos N mensajes completos en el contexto.
   */
  history: {
    /**
     * Mensajes incluidos en el contexto de chat
     *
     * Valor actual: 10 mensajes (5 pares usuario-asistente)
     *
     * Usado en: app/api/chat/stream/route.ts
     *
     * Se obtienen de DB con:
     *   prisma.message.findMany({ take: 10, orderBy: { timestamp: 'desc' } })
     *
     * Trade-offs:
     * - MÃ¡s mensajes:
     *   âœ… Mejor coherencia en conversaciones largas
     *   âœ… Claude recuerda contexto anterior
     *   âŒ MÃ¡s input tokens = mÃ¡s costo
     *   âŒ Mayor latencia
     *
     * - Menos mensajes:
     *   âœ… Menor costo y latencia
     *   âŒ Pierde coherencia si conversaciÃ³n es larga
     *   âŒ Usuario debe repetir contexto
     *
     * Benchmarks:
     * - 5 mensajes: Conversaciones muy cortas (2-3 intercambios)
     * - 10 mensajes: Balance Ã³ptimo (5 intercambios) âœ…
     * - 20 mensajes: Conversaciones complejas (mucho contexto)
     *
     * ESTIMACIÃ“N DE TOKENS:
     * 10 mensajes â‰ˆ 1000-1500 input tokens adicionales
     * 6 mensajes â‰ˆ 600-900 input tokens adicionales
     */
    chatContext: 6,

    /**
     * Mensajes en contexto de verificaciÃ³n
     *
     * Valor actual: 0 (historial ELIMINADO en optimizaciÃ³n)
     *
     * CAMBIO RECIENTE (2025-01-11):
     * Antes: 5 mensajes incluidos en verification prompt
     * Ahora: 0 mensajes (eliminado completamente)
     *
     * RazÃ³n: OptimizaciÃ³n de tokens (~200-500 tokens ahorrados por verificaciÃ³n)
     *
     * Impacto:
     * âœ… 95% de casos: Sin impacto (respuestas autÃ³nomas)
     * âš ï¸ 5% de casos: Menor precisiÃ³n en referencias contextuales
     *    Ejemplo: "lo que mencionaste antes" sin contexto explÃ­cito
     *
     * MEMORIA DEL SISTEMA:
     * Aunque verification no tiene historial, Claude SÃ lo tiene vÃ­a
     * messages array, por lo que la conversaciÃ³n mantiene coherencia.
     */
    verificationContext: 0,
  },

  /**
   * SISTEMA DE HINTS PROGRESIVOS
   *
   * Ayuda adaptativa basada en intentos fallidos del estudiante.
   *
   * FilosofÃ­a pedagÃ³gica:
   * - No dar respuesta directa inmediatamente (mÃ©todo socrÃ¡tico)
   * - Ofrecer hints progresivamente segÃºn dificultad
   * - Balance entre challenge y frustraciÃ³n
   */
  hints: {
    /**
     * Intentos mÃ­nimos antes de mostrar primer hint
     *
     * Valor actual: 2 intentos
     *
     * ProgresiÃ³n:
     * - Intento 1: Sin hint (estudiante debe intentar solo)
     * - Intento 2: Sin hint (segunda oportunidad)
     * - Intento 3+: Comienzan hints progresivos
     *
     * Usado en: lib/prompt-builder.ts
     */
    minAttempts: 2,

    /**
     * Frecuencia de nuevos hints (cada N intentos)
     *
     * Valor actual: 2 intentos
     *
     * ProgresiÃ³n de hints:
     * - Intento 2: Hint 0 (primer hint, mÃ¡s sutil)
     * - Intento 4: Hint 1 (segundo hint, mÃ¡s directo)
     * - Intento 6: Hint 2 (tercer hint, casi explÃ­cito)
     *
     * FÃ³rmula: hintIndex = Math.floor(attempts / frequency) - 1
     *
     * Usado en:
     * - lib/prompt-builder.ts (mostrar en system prompt)
     * - lib/activity-verification.ts (determinar si mostrar)
     */
    frequency: 2,
  },

  /**
   * TIMEOUTS Y DURACIONES
   *
   * LÃ­mites de tiempo para prevenir timeouts en Vercel y mala UX.
   */
  timeouts: {
    /**
     * Timeout mÃ¡ximo para API routes (Vercel)
     *
     * Valor actual: 60 segundos
     *
     * LÃ­mites de Vercel:
     * - Hobby plan: 10 segundos (muy restrictivo)
     * - Pro plan: 60 segundos âœ…
     * - Enterprise: 900 segundos (15 min)
     *
     * IMPORTANTE: Streaming permite respuestas largas sin timeout
     * porque el connection se mantiene abierta con chunks incrementales.
     *
     * âš ï¸ LIMITACIÃ“N TÃ‰CNICA:
     * Next.js requiere que `export const maxDuration` sea un valor literal,
     * NO puede ser una expresiÃ³n runtime. Por lo tanto, este valor estÃ¡
     * duplicado como literal en app/api/chat/stream/route.ts:18
     *
     * Si cambias este valor, DEBES actualizar manualmente el literal.
     *
     * Usado en:
     * - app/api/chat/stream/route.ts (lÃ­nea 18, duplicado como literal)
     */
    vercelMaxDuration: 60,
  },

  /**
   * POLLING Y ACTUALIZACIONES DE UI
   *
   * Frecuencia con la que el frontend consulta el backend por actualizaciones.
   *
   * Trade-offs:
   * - Polling rÃ¡pido (<3s): MÃ¡s reactivo pero mÃ¡s requests al servidor
   * - Polling lento (>10s): Menos overhead pero UX menos fluida
   */
  polling: {
    /**
     * Intervalo de polling para barra de progreso
     *
     * Valor actual: 3000 ms (3 segundos)
     *
     * Contexto: ActualizaciÃ³n de barra de progreso en header y chat
     *
     * Usado en:
     * - components/learning/activity-progress-header.tsx
     * - components/learning/chat-interface.tsx
     *
     * NOTA: Con arquitectura actual, el progreso se actualiza
     * instantÃ¡neamente vÃ­a SSE events (activity_completed).
     * Este polling es FALLBACK por si el evento SSE falla.
     *
     * TODO: Considerar eliminar polling y confiar 100% en SSE events.
     */
    progressIntervalMs: 3000,
  },

  /**
   * CONFIGURACIONES ADICIONALES (menos crÃ­ticas)
   */
  fallback: {
    /**
     * Longitud mÃ­nima de keywords en verificaciÃ³n fallback
     *
     * Valor actual: 4 caracteres
     *
     * Usado en: lib/activity-verification.ts
     *
     * Contexto: Si la IA falla, se hace verificaciÃ³n simple por keywords.
     * Solo considera palabras con mÃ¡s de N caracteres para evitar
     * falsos positivos con palabras comunes ("es", "un", "el").
     *
     * Este fallback rara vez se usa (solo si Anthropic API falla).
     */
    keywordMinLength: 4,
  },
}

/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * TIPO EXPORTADO (para TypeScript)
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
export type AIConfig = typeof AI_CONFIG

/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * CHANGELOG
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 *
 * 2025-01-11: CreaciÃ³n del archivo ai-config.ts
 *   - Centralizar todas las configuraciones AI hardcodeadas
 *   - Eliminar historial de verification prompt (optimizaciÃ³n de tokens)
 *   - Documentar arquitectura de memoria del sistema
 *
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 */
